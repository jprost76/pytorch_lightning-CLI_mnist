# pytorch_lightning==2.0.2
fit: &fit
  seed_everything: true
  trainer:
    callbacks:
      - class_path: pytorch_lightning.callbacks.EarlyStopping
        init_args:
          patience: 5
          monitor: valid_loss
      - class_path: pytorch_lightning.callbacks.LearningRateMonitor
        init_args:
          logging_interval: step
      # demo on how to use a custom callback (not actually usefull)
      - class_path: callbacks.print.MyPrintingCallback
        init_args:
          p: 10
    accelerator: auto
    strategy: auto
    devices: auto
    num_nodes: 1
    precision: 32-true
    logger: true
    max_epochs: 10
    val_check_interval: null
    check_val_every_n_epoch: 1
    num_sanity_val_steps: 1
    log_every_n_steps: 10
  optimizer:
    class_path: torch.optim.Adam
    init_args:
      lr: 0.01
      betas: [0.9, 0.999]
  lr_scheduler: 
    class_path: pytorch_lightning.cli.ReduceLROnPlateau
    init_args:
      factor: 0.1
      patience: 2
      threshold: 0.01
      threshold_mode: rel
      monitor: valid_loss
  model:
    network:
      class_path: networks.nets.TwoLayerNet
      init_args:
        hidden_dim: 12
        out_dim: 10
    learning_rate: 0.02
  data:
    class_path: data.MNISTDataModule
    init_args:
      data_dir: datasets
      batch_size: 4
  ckpt_path: null